{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 Session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset loading and preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open('Dataset/sentiment.txt','r')\n",
    "sentences=f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "stopwords = [\"a\", \"about\", \"above\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\", \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\",\"although\",\"always\",\"am\",\"among\", \"amongst\", \"amoungst\", \"amount\",  \"an\", \"and\", \"another\", \"any\",\"anyhow\",\"anyone\",\"anything\",\"anyway\", \"anywhere\", \"are\", \"around\", \"as\",  \"at\", \"back\",\"be\",\"became\", \"because\",\"become\",\"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\", \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\", \"bottom\",\"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\", \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\", \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\",\"else\", \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\", \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fify\", \"fill\", \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\", \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\", \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"however\", \"hundred\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\", \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\", \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\", \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\", \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\", \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\", \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\",\"part\", \"per\", \"perhaps\", \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\", \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\", \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\", \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\", \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\", \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thickv\", \"thin\", \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\", \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\", \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\", \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\", \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\", \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\", \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\", \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"the\"]\n",
    "\n",
    "def text_cleaner(text): \n",
    "    text=remove_link(text.lower())\n",
    "    long_words=[]\n",
    "    for i in text.split():\n",
    "        if i not in stopwords:                  \n",
    "            long_words.append(i)\n",
    "    return long_words\n",
    "\n",
    "def remove_link(text):\n",
    "    regex = r'https?://[^\\s<>)\"‘’]+'\n",
    "    match = re.sub(regex,' ', text)\n",
    "    regex = r'https?:|urls?|[/\\:,-.\"\\'?!;…]+'\n",
    "    tweet = re.sub(regex,' ', match)\n",
    "    tweet = re.sub(\"[^a-zA-Z_]\", \" \", tweet)\n",
    "    tweet = re.sub(\"[ ]+\", \" \", tweet) \n",
    "    return tweet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary=[]\n",
    "X=[]\n",
    "y=[]\n",
    "for line in sentences:\n",
    "\ttexts=line.strip().lower().split('\\t')\n",
    "\ttokens=text_cleaner(texts[0])\n",
    "\ty.append(texts[1])\n",
    "\tX.append(tokens)\n",
    "\tfor word in tokens:\n",
    "\t\tvocabulary.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion of text to vector space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def t2v(tokens,vocabulary):\n",
    "\tvect=[]\n",
    "\tfor feature in vocabulary:\n",
    "\t\tif feature in tokens:\n",
    "\t\t\tvect.append(1)\n",
    "\t\telse:\n",
    "\t\t\tvect.append(0)\n",
    "\treturn vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector=[]\n",
    "for tokens in X:\n",
    "\tvect=t2v(tokens,vocabulary)\n",
    "\tvector.append(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples,Number of features\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1143, 12213)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector=np.asarray(vector)\n",
    "y=np.asarray(y)\n",
    "print(\"Number of samples,Number of features\")\n",
    "vector.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12213,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "X_new = mutual_info_classif(vector,y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4333"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N=len(vocabulary)\n",
    "avg=0\n",
    "for mi in X_new:\n",
    "    avg+=mi\n",
    "avg=avg/N\n",
    "feat_selection=[]\n",
    "for i,mi in enumerate(X_new):\n",
    "    if mi>avg:\n",
    "        feat_selection.append(vocabulary[i])\n",
    "len(feat_selection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['humbles',\n",
       " 'exalted',\n",
       " 'semst',\n",
       " 'sought',\n",
       " 'truth',\n",
       " 'stand',\n",
       " 'world',\n",
       " 'tight',\n",
       " 'hug',\n",
       " 'relieve',\n",
       " 'morality',\n",
       " 'hitch',\n",
       " 'semst',\n",
       " 'godly',\n",
       " 'trusts',\n",
       " 'wants',\n",
       " 'god',\n",
       " 'day',\n",
       " 'wisdom',\n",
       " 'sense',\n",
       " 'semst',\n",
       " 'days',\n",
       " 'freethinker',\n",
       " 'v',\n",
       " 'deen',\n",
       " 'instead',\n",
       " 'strongly',\n",
       " 'faith',\n",
       " 'martin',\n",
       " 'semst',\n",
       " 'fears',\n",
       " 'god',\n",
       " 'god',\n",
       " 'semst',\n",
       " 'pray',\n",
       " 'catholic',\n",
       " 'semst',\n",
       " 'life',\n",
       " 'faith',\n",
       " 'inevitable',\n",
       " 'henri',\n",
       " 'semst',\n",
       " 'semst',\n",
       " 'i',\n",
       " 'semst',\n",
       " 'father',\n",
       " 'vessel',\n",
       " 'ur',\n",
       " 'sundaymorningshow',\n",
       " 'semst',\n",
       " 'remember',\n",
       " 'prayed',\n",
       " 'god',\n",
       " 'god',\n",
       " 'strong',\n",
       " 'thank',\n",
       " 'god',\n",
       " 'god',\n",
       " 'jesus',\n",
       " 'religious',\n",
       " 'environment',\n",
       " 'create',\n",
       " 'royal',\n",
       " 'environment',\n",
       " 'thank',\n",
       " 'god',\n",
       " 'sanctify',\n",
       " 'word',\n",
       " 'truth',\n",
       " 'john',\n",
       " 'semst',\n",
       " 'psalm',\n",
       " 'sign',\n",
       " 'humanist',\n",
       " 'cloth',\n",
       " 'tcot',\n",
       " 'stay',\n",
       " 'love',\n",
       " 'tcot',\n",
       " 'semst',\n",
       " 'pray',\n",
       " 'won',\n",
       " 'sees',\n",
       " 'difficulties',\n",
       " 'counts',\n",
       " 'god',\n",
       " 'semst',\n",
       " 'beast',\n",
       " 'school',\n",
       " 'secular',\n",
       " 'secularism',\n",
       " 'courts',\n",
       " 'jesus',\n",
       " 'believe',\n",
       " 'died',\n",
       " 'conflict',\n",
       " 'god',\n",
       " 'proud',\n",
       " 'imran',\n",
       " 's',\n",
       " 'pakistan',\n",
       " 'inclusive',\n",
       " 'semst',\n",
       " 'moving',\n",
       " 'people',\n",
       " 'i',\n",
       " 'nonreligious',\n",
       " 'weddings',\n",
       " 'student',\n",
       " 'discounts',\n",
       " 'rt',\n",
       " 'right',\n",
       " 'exist',\n",
       " 'sphere',\n",
       " 'semst',\n",
       " 'gmhumanistchoir',\n",
       " 'looking',\n",
       " 'join',\n",
       " 'auditions',\n",
       " 'manchest',\n",
       " 'ps',\n",
       " 'god',\n",
       " 'want',\n",
       " 'actions',\n",
       " 'spiritual',\n",
       " 'deeper',\n",
       " 'silence',\n",
       " 'belief',\n",
       " 'semst',\n",
       " 'diseases',\n",
       " 'acatholicprayer',\n",
       " 'god',\n",
       " 'semst',\n",
       " 'realize',\n",
       " 'builds',\n",
       " 'understand',\n",
       " 'god',\n",
       " 'emunah',\n",
       " 'semst',\n",
       " 'eager',\n",
       " 'semst',\n",
       " 'faith',\n",
       " 'received',\n",
       " 'did',\n",
       " 'repentance',\n",
       " 'atonement',\n",
       " 'god',\n",
       " 'life',\n",
       " 'impossible',\n",
       " 'remain',\n",
       " 'desolate',\n",
       " 'place',\n",
       " 'newthings',\n",
       " 'heaven',\n",
       " 'words',\n",
       " 'pass',\n",
       " 'matthew',\n",
       " 'semst',\n",
       " 'impossible',\n",
       " 'i',\n",
       " 'loved',\n",
       " 'lord',\n",
       " 'feared',\n",
       " 'come',\n",
       " 'i',\n",
       " 'happened',\n",
       " 'fear',\n",
       " 'jesus',\n",
       " 'helper',\n",
       " 'trust',\n",
       " 'finished',\n",
       " 'cross',\n",
       " 'amen',\n",
       " 'lord',\n",
       " 'effort',\n",
       " 'tiny',\n",
       " 'ones',\n",
       " 'like',\n",
       " 'lds',\n",
       " 'inspiration',\n",
       " 'semst',\n",
       " 'live',\n",
       " 'change',\n",
       " 'places',\n",
       " 'offered',\n",
       " 'right',\n",
       " 'god',\n",
       " 'truest',\n",
       " 'path',\n",
       " 'love',\n",
       " 'god',\n",
       " 'worked',\n",
       " 'define',\n",
       " 'existentialism',\n",
       " 'semst',\n",
       " 'm',\n",
       " 'making',\n",
       " 'tho',\n",
       " 'o',\n",
       " 'god',\n",
       " 'patience',\n",
       " 'priorities',\n",
       " 'bend',\n",
       " 'freelance',\n",
       " 'gigs',\n",
       " 'pdx',\n",
       " 'intention',\n",
       " 'i',\n",
       " 'huge',\n",
       " 'thofjuly',\n",
       " 'planetearth',\n",
       " 'heart',\n",
       " 'glad',\n",
       " 'answer',\n",
       " 'reproacheth',\n",
       " 'great',\n",
       " 'time',\n",
       " 'sunday',\n",
       " 'bridgechurch',\n",
       " 'weloveyou',\n",
       " 'semst',\n",
       " 'good',\n",
       " 'teaches',\n",
       " 'good',\n",
       " 'semst',\n",
       " 's',\n",
       " 'like',\n",
       " 'test',\n",
       " 'sort',\n",
       " 'best',\n",
       " 'unfit',\n",
       " 'adversity',\n",
       " 'worship',\n",
       " 'semst',\n",
       " 'steppin',\n",
       " 'safe',\n",
       " 'blessed',\n",
       " 'semst',\n",
       " 'isaiah',\n",
       " 'i',\n",
       " 'holy',\n",
       " 'semst',\n",
       " 'peace',\n",
       " 'lead',\n",
       " 'rock',\n",
       " 'higher',\n",
       " 'psalm',\n",
       " 'semst',\n",
       " 'active',\n",
       " 'love',\n",
       " 'trust',\n",
       " 'manning',\n",
       " 'i',\n",
       " 'forever',\n",
       " 't',\n",
       " 'o',\n",
       " 'god',\n",
       " 'like',\n",
       " 'saint',\n",
       " 'neri',\n",
       " 'semst',\n",
       " 'jer',\n",
       " 'god',\n",
       " 'love',\n",
       " 'great',\n",
       " 'lord',\n",
       " 'god',\n",
       " 'eritrea',\n",
       " 'semst',\n",
       " 'eyes',\n",
       " 'god',\n",
       " 'difficulties',\n",
       " 'precious',\n",
       " 'awakeningeurope',\n",
       " 'paulyman',\n",
       " 'heidsbakes',\n",
       " 'jc',\n",
       " 'movingdisaster',\n",
       " 'roomansi',\n",
       " 'obey',\n",
       " 'acts',\n",
       " 'countries',\n",
       " 'better',\n",
       " 'semst',\n",
       " 'follow',\n",
       " 'christ',\n",
       " 'church',\n",
       " 'trinity',\n",
       " 'usa',\n",
       " 'uk',\n",
       " 'semst',\n",
       " 'resting',\n",
       " 'god',\n",
       " 'unknow',\n",
       " 'bible',\n",
       " 'scripture',\n",
       " 'semst',\n",
       " 'wives',\n",
       " 'christ',\n",
       " 'esv',\n",
       " 'semst',\n",
       " 'devil',\n",
       " 'lord',\n",
       " 'open',\n",
       " 'spencer',\n",
       " 'w',\n",
       " 'hope',\n",
       " 'rt',\n",
       " 'love',\n",
       " 'gospel',\n",
       " 'soul',\n",
       " 'spirits',\n",
       " 'hindering',\n",
       " 'good',\n",
       " 'coming',\n",
       " 'lord',\n",
       " 'let',\n",
       " 'disappear',\n",
       " 'pseud',\n",
       " 'awareness',\n",
       " 'compassion',\n",
       " 'semst',\n",
       " 'sovereign',\n",
       " 'lord',\n",
       " 'amos',\n",
       " 'loving',\n",
       " 'i',\n",
       " 'motivated',\n",
       " 'semst',\n",
       " 'proverbs',\n",
       " 'kjv',\n",
       " 'evil',\n",
       " 'men',\n",
       " 'lord',\n",
       " 'live',\n",
       " 'believe',\n",
       " 'existence',\n",
       " 'test',\n",
       " 'prayers',\n",
       " 'lord',\n",
       " 'jesus',\n",
       " 'hope',\n",
       " 'like',\n",
       " 'welcome',\n",
       " 'lucy',\n",
       " 'dee',\n",
       " 'atheist',\n",
       " 'apocalypse',\n",
       " 'god',\n",
       " 'god',\n",
       " 'mercy',\n",
       " 'life',\n",
       " 'certainly',\n",
       " 'god',\n",
       " 'james',\n",
       " 'skeptic',\n",
       " 'semst',\n",
       " 'god',\n",
       " 'loving',\n",
       " 'lord',\n",
       " 'bring',\n",
       " 'cure',\n",
       " 'abundance',\n",
       " 'love',\n",
       " 'loved',\n",
       " 'god',\n",
       " 'john',\n",
       " 'love',\n",
       " 'say',\n",
       " 'semst',\n",
       " 'bless',\n",
       " 'semst',\n",
       " 'decree',\n",
       " 'thou',\n",
       " 'thou',\n",
       " 'life',\n",
       " 'world',\n",
       " 'shepherd',\n",
       " 'good',\n",
       " 'shepherd',\n",
       " 'lays',\n",
       " 'god',\n",
       " 'hallelujah',\n",
       " 'amen',\n",
       " 'really',\n",
       " 'incredible',\n",
       " 'semst',\n",
       " 'god',\n",
       " 'brought',\n",
       " 'just',\n",
       " 'sent',\n",
       " 'direction',\n",
       " 'follow',\n",
       " 't',\n",
       " 'thinkandgrowrich',\n",
       " 'people',\n",
       " 'walk',\n",
       " 'darkness',\n",
       " 'land',\n",
       " 'isaiah',\n",
       " 'lord',\n",
       " 'things',\n",
       " 'gift',\n",
       " 'regrets',\n",
       " 'amwriting',\n",
       " 'president',\n",
       " 'ilham',\n",
       " 'aliyev',\n",
       " 's',\n",
       " 'pray',\n",
       " 'church',\n",
       " 'hospital',\n",
       " 'christ',\n",
       " 'semst',\n",
       " 'gifts',\n",
       " 'started',\n",
       " 'studying',\n",
       " 'thank',\n",
       " 'semst',\n",
       " 'pray',\n",
       " 'best',\n",
       " 'desires',\n",
       " 'god',\n",
       " 'placed',\n",
       " 'destiny',\n",
       " 'allah',\n",
       " 'gods',\n",
       " 'love',\n",
       " 'humanity',\n",
       " 'real',\n",
       " 'love',\n",
       " 'pretty',\n",
       " 'cancered',\n",
       " 'kid',\n",
       " 'semst',\n",
       " 'parcel',\n",
       " 'god',\n",
       " 'god',\n",
       " 'things',\n",
       " 'god',\n",
       " 'semst',\n",
       " 'holy',\n",
       " 'god',\n",
       " 'death',\n",
       " 'amen',\n",
       " 'teamjesus',\n",
       " 'god',\n",
       " 'spend',\n",
       " 'don',\n",
       " 'god',\n",
       " 'afraid',\n",
       " 'god',\n",
       " 'setting',\n",
       " 'humble',\n",
       " 'god',\n",
       " 'confident',\n",
       " 'lord',\n",
       " 'happy',\n",
       " 'blessed',\n",
       " 'semst',\n",
       " 'plutoflyby',\n",
       " 'pluto',\n",
       " 'endreligion',\n",
       " 'semst',\n",
       " 'grubb',\n",
       " 'energy',\n",
       " 'net',\n",
       " 'importer',\n",
       " 'ccfc',\n",
       " 'journey',\n",
       " 'speaker',\n",
       " 'van',\n",
       " 'experience',\n",
       " 'help',\n",
       " 'climate',\n",
       " 'climwarn',\n",
       " 'cfcc',\n",
       " 'need',\n",
       " 'fossilfuel',\n",
       " 'change',\n",
       " 'gradual',\n",
       " 'lets',\n",
       " 'semst',\n",
       " 't',\n",
       " 'parents',\n",
       " 'borrow',\n",
       " 'conservation',\n",
       " 'perspective',\n",
       " 'vj',\n",
       " 'd',\n",
       " 'convert',\n",
       " 'solar',\n",
       " 'actonclimate',\n",
       " 'richer',\n",
       " 'countries',\n",
       " 'emissions',\n",
       " 'poor',\n",
       " 'countries',\n",
       " 'transition',\n",
       " 'climate',\n",
       " 'purchase',\n",
       " 'renewable',\n",
       " 'power',\n",
       " 'utility',\n",
       " 'avekathleen',\n",
       " 'ecologyaction',\n",
       " 'carbonpricing',\n",
       " 'semst',\n",
       " 's',\n",
       " 'exciting',\n",
       " 'witness',\n",
       " 'major',\n",
       " 'development',\n",
       " 'urgenda',\n",
       " 'chadcowie',\n",
       " 's',\n",
       " 'semst',\n",
       " 'change',\n",
       " 'tweet',\n",
       " 'tips',\n",
       " 'inspire',\n",
       " 'climate',\n",
       " 'zim',\n",
       " 'semst',\n",
       " 'counting',\n",
       " 'tephrochronology',\n",
       " 'chronology',\n",
       " 'oxford',\n",
       " 'solutions',\n",
       " 'city',\n",
       " 'semst',\n",
       " 'catholic',\n",
       " 'semst',\n",
       " 'adaptation',\n",
       " 'reduces',\n",
       " 'negative',\n",
       " 'impact',\n",
       " 'difficult',\n",
       " 'alongside',\n",
       " 'cfcc',\n",
       " 'countries',\n",
       " 'forefront',\n",
       " 'consequences',\n",
       " 'change',\n",
       " 'discussion',\n",
       " 'radio',\n",
       " 'revcindy',\n",
       " 'affecting',\n",
       " 'encyclical',\n",
       " 'kind',\n",
       " 'feet',\n",
       " 'bbcradio',\n",
       " 'semst',\n",
       " 'participation',\n",
       " 'essential',\n",
       " 'bring',\n",
       " 'agcop',\n",
       " 'semst',\n",
       " 'kca',\n",
       " 'votejkt',\n",
       " 'climatecommrpb',\n",
       " 'green',\n",
       " 'healthy',\n",
       " 'planet',\n",
       " 'semst',\n",
       " 'thank',\n",
       " 'follow',\n",
       " 'i',\n",
       " 'm',\n",
       " 'semst',\n",
       " 'leader',\n",
       " 'trending',\n",
       " 'canada',\n",
       " 'tweets',\n",
       " 'climate',\n",
       " 'sm',\n",
       " 'semst',\n",
       " 'thanks',\n",
       " 'maine',\n",
       " 'support',\n",
       " 'energyefficiency',\n",
       " 'committed',\n",
       " 'green',\n",
       " 'dreams',\n",
       " 'change',\n",
       " 'parents',\n",
       " 'grandparents',\n",
       " 'csota',\n",
       " 'time',\n",
       " 'drink',\n",
       " 'rt',\n",
       " 'presentation',\n",
       " 'anzca',\n",
       " 'semst',\n",
       " 'degrees',\n",
       " 'love',\n",
       " 'survive',\n",
       " 'climate',\n",
       " 'infographic',\n",
       " 'water',\n",
       " 'mission',\n",
       " 'climate',\n",
       " 'road',\n",
       " 'greenhouse',\n",
       " 'vehicle',\n",
       " 'missionaccept',\n",
       " 'win',\n",
       " 'csota',\n",
       " 'moving',\n",
       " 'scientist',\n",
       " 'bbcradio',\n",
       " 's',\n",
       " 'future',\n",
       " 'following',\n",
       " 'summit',\n",
       " 'climate',\n",
       " 'semst',\n",
       " 'voices',\n",
       " 'say',\n",
       " 'cop',\n",
       " 'walshconstco',\n",
       " 'orbizclimatedeclaration',\n",
       " 'fighting',\n",
       " 'climate',\n",
       " 'change',\n",
       " 'rettet',\n",
       " 'die',\n",
       " 'earth',\n",
       " 'bbcweather',\n",
       " 'global',\n",
       " 'thanks',\n",
       " 'semst',\n",
       " 'corporations',\n",
       " 'semst',\n",
       " 'mission',\n",
       " 'climate',\n",
       " 'shower',\n",
       " 'instead',\n",
       " 'tip',\n",
       " 'loveourplanet',\n",
       " 'recipe',\n",
       " 'warmer',\n",
       " 'year',\n",
       " 'maybe',\n",
       " 'footprint',\n",
       " 'sooner',\n",
       " 'later',\n",
       " 'wind',\n",
       " 'power',\n",
       " 'snap',\n",
       " 'increased',\n",
       " 'climate',\n",
       " 'semst',\n",
       " 'feminists',\n",
       " 't',\n",
       " 'tell',\n",
       " 'i',\n",
       " 'girl',\n",
       " 'wearing',\n",
       " 'mustache',\n",
       " 'machine',\n",
       " 'semst',\n",
       " 'know',\n",
       " 'maybelline',\n",
       " 'beauty',\n",
       " 'i',\n",
       " 'great',\n",
       " 'rack',\n",
       " 'felt',\n",
       " 'going',\n",
       " 'way',\n",
       " 'gayrights',\n",
       " 'always__already',\n",
       " 'fantastic',\n",
       " 'captured',\n",
       " 't',\n",
       " 'missed',\n",
       " 'point',\n",
       " 'kya',\n",
       " 'enjoyed',\n",
       " 'tahirrajbhasin',\n",
       " 'p',\n",
       " 'uterus',\n",
       " 'shebelieves',\n",
       " 'semst',\n",
       " 'people',\n",
       " 'hope',\n",
       " 'solo',\n",
       " 'abuse',\n",
       " 'i',\n",
       " 'house',\n",
       " 'won',\n",
       " 'world',\n",
       " 'brokenlegprobs',\n",
       " 'usa',\n",
       " 'want',\n",
       " 'test',\n",
       " 'written',\n",
       " 'self',\n",
       " 'woman',\n",
       " 'body',\n",
       " 'confiscated',\n",
       " 'cixous',\n",
       " 'fabulous',\n",
       " 'feminist',\n",
       " 'birthday',\n",
       " 'queen',\n",
       " 'frida',\n",
       " 'kahlo',\n",
       " 'mindstimulation',\n",
       " 'latinapower',\n",
       " 'mustangallie_',\n",
       " 'i',\n",
       " 'watched',\n",
       " 'awesome',\n",
       " 'caitlinmoran',\n",
       " 'ladies',\n",
       " 'semst',\n",
       " 'shannonbream',\n",
       " 'review',\n",
       " 'thanks',\n",
       " 'report',\n",
       " 'excellent',\n",
       " 'yo',\n",
       " 'kaitlynbristowe',\n",
       " 'thebachelorette',\n",
       " 'semst',\n",
       " 'women',\n",
       " 'rights',\n",
       " 'human',\n",
       " 'bad',\n",
       " 'good',\n",
       " 'equality',\n",
       " 'contradictions',\n",
       " 'demonization',\n",
       " 'agenda',\n",
       " 'semst',\n",
       " 's',\n",
       " 'oral',\n",
       " 'film',\n",
       " 'seriously',\n",
       " 'secondary',\n",
       " 'school',\n",
       " 'thoughts',\n",
       " 'semst',\n",
       " 'nero',\n",
       " 'author',\n",
       " 'onepoundstories',\n",
       " 's',\n",
       " 'money',\n",
       " 'moo_mena',\n",
       " 'unlearn',\n",
       " 'fab',\n",
       " 'change',\n",
       " 'semst',\n",
       " 'rt',\n",
       " 'need',\n",
       " 'know',\n",
       " 'semst',\n",
       " 'allybrooke',\n",
       " 'u',\n",
       " 'people',\n",
       " 'need',\n",
       " 'know',\n",
       " 'feminist',\n",
       " 'woman',\n",
       " 'airport',\n",
       " 'i',\n",
       " 'did',\n",
       " 'step',\n",
       " 'partner',\n",
       " 'grabbed',\n",
       " 'threatened',\n",
       " 'yesallwomen',\n",
       " 'semst',\n",
       " 'like',\n",
       " 'tho',\n",
       " 'clicked',\n",
       " 'improve',\n",
       " 'spell',\n",
       " 'andywarhol',\n",
       " 'anamendieta',\n",
       " 'semst',\n",
       " 'great',\n",
       " 'progress',\n",
       " 'antiquated',\n",
       " 'semst',\n",
       " 'birthday',\n",
       " 'people',\n",
       " 'fellow',\n",
       " 'i',\n",
       " 'love',\n",
       " 'semst',\n",
       " 'don',\n",
       " 't',\n",
       " 'tell',\n",
       " 'girls',\n",
       " 'rock',\n",
       " 'shit',\n",
       " 'pearson',\n",
       " 'semst',\n",
       " 'i',\n",
       " 'i',\n",
       " 'housewife',\n",
       " 'househusband',\n",
       " 'seemsfun',\n",
       " 'semst',\n",
       " 'abouttime',\n",
       " 'women',\n",
       " 'earth',\n",
       " 'deal',\n",
       " 'george',\n",
       " 'carlin',\n",
       " 'semst',\n",
       " 's',\n",
       " 'pirate',\n",
       " 'skit',\n",
       " 've',\n",
       " 'defeated',\n",
       " 'fazle',\n",
       " 'womenempowerment',\n",
       " 'quote',\n",
       " 'quoteoftheday',\n",
       " 'gender',\n",
       " 'icons',\n",
       " 'women',\n",
       " 'officially',\n",
       " 'man',\n",
       " 'wrote',\n",
       " 'vital',\n",
       " 'ends',\n",
       " 'alimony',\n",
       " 'soon',\n",
       " 'divorce',\n",
       " 'leanin',\n",
       " 'fcking',\n",
       " 'womenaredoctorstoo',\n",
       " 'semst',\n",
       " 's',\n",
       " 'ffd',\n",
       " 'happening',\n",
       " 'follow',\n",
       " 'femnetprog',\n",
       " 'semst',\n",
       " 'im',\n",
       " 'okay',\n",
       " 'great',\n",
       " 'people',\n",
       " 'healers',\n",
       " 'elizabeth',\n",
       " 'quote',\n",
       " 'follow',\n",
       " 'tumblrposts',\n",
       " 'thankyouellenpao',\n",
       " 'semst',\n",
       " 'don',\n",
       " 'nerds',\n",
       " 'semst',\n",
       " 'resist',\n",
       " 'don',\n",
       " 't',\n",
       " 'bother',\n",
       " 'maisie_williams',\n",
       " 'campaign',\n",
       " 'brilliant',\n",
       " 'badass',\n",
       " 'just',\n",
       " 'like',\n",
       " 'resume',\n",
       " 'i',\n",
       " 'yrs',\n",
       " 'woman',\n",
       " 'president',\n",
       " 'semst',\n",
       " 'believing',\n",
       " 'believe',\n",
       " 'government',\n",
       " 'create',\n",
       " 'created',\n",
       " 'government',\n",
       " 'ronald',\n",
       " 'semst',\n",
       " 'irishsmiles',\n",
       " 'makedclisten',\n",
       " 'nolibprogs',\n",
       " 'women',\n",
       " 'soccer',\n",
       " 'winning',\n",
       " 'women',\n",
       " 'rule',\n",
       " 'freeallfour',\n",
       " 'usa',\n",
       " 'showed',\n",
       " 'world',\n",
       " 'hillary',\n",
       " 'wwc',\n",
       " 'fearless',\n",
       " 'chasing',\n",
       " 'team',\n",
       " 'h',\n",
       " 'finds',\n",
       " 'hrc',\n",
       " 'horrible',\n",
       " 'rt',\n",
       " 'cattyhipster',\n",
       " 'like',\n",
       " 's',\n",
       " 'day',\n",
       " 'bills',\n",
       " 'appropriately',\n",
       " 'just',\n",
       " 'case',\n",
       " 'isn',\n",
       " 't',\n",
       " 'semst',\n",
       " 's',\n",
       " 'true',\n",
       " 'long',\n",
       " 'hard',\n",
       " 'accomplishment',\n",
       " 'got',\n",
       " 'finances',\n",
       " 'spend',\n",
       " 'save',\n",
       " 'day',\n",
       " 'live',\n",
       " 'means',\n",
       " 'greece',\n",
       " 'pantanolaw',\n",
       " 'thanks',\n",
       " 'god',\n",
       " 'bless',\n",
       " 'freeallfour',\n",
       " 'semst',\n",
       " 'shirt',\n",
       " 'w',\n",
       " 'lots',\n",
       " 'glitter',\n",
       " 'amen',\n",
       " 'semst',\n",
       " 'uniteblue',\n",
       " 'feelthebern',\n",
       " 'semst',\n",
       " 'good',\n",
       " 'need',\n",
       " 'nohillary',\n",
       " 'semst',\n",
       " 'adamsmith_usa',\n",
       " 'video',\n",
       " 'luck',\n",
       " 'nc',\n",
       " 'semst',\n",
       " 'm',\n",
       " 'depending',\n",
       " 'tweet',\n",
       " 'hillaryclinton',\n",
       " 'time',\n",
       " 'good',\n",
       " 'good',\n",
       " 'sound',\n",
       " 'sound',\n",
       " 'solutions',\n",
       " 'readyforhillary',\n",
       " 'semst',\n",
       " 'christians',\n",
       " 'luke',\n",
       " 'john',\n",
       " 'party',\n",
       " 'anti',\n",
       " 'follow',\n",
       " 'hillaryclinton',\n",
       " 'dalailama',\n",
       " 'women',\n",
       " 'leadership',\n",
       " 'thanks',\n",
       " 'follow',\n",
       " 'based',\n",
       " 'thought',\n",
       " 'free',\n",
       " 'day',\n",
       " 'i',\n",
       " 'quitter',\n",
       " 'hillaryforia',\n",
       " 'hillaryclinton',\n",
       " 's',\n",
       " 'campaign',\n",
       " 'hillaryclinton',\n",
       " 'answers',\n",
       " 'revolving',\n",
       " 'leighannkopans',\n",
       " 'i',\n",
       " 'pattern',\n",
       " 'cross',\n",
       " 'stitch',\n",
       " 'i',\n",
       " 'want',\n",
       " 'make',\n",
       " 'semst',\n",
       " 'clinton',\n",
       " 'accomplishment',\n",
       " ...]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_feat=[]\n",
    "for tokens in X:\n",
    "\tvect=t2v(tokens,feat_selection)\n",
    "\tvector_feat.append(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1143, 4333)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "vector_feat=np.asarray(vector_feat)\n",
    "vector_feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'watching', 'netflix', 'original', 'movies']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip=\"I love watching netflix original movies\"\n",
    "tokens=text_cleaner(test_ip)\n",
    "vect_pos=t2v(tokens,feat_selection)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'hate', 'watching', 'netflix', 'original', 'movies']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip=\"I hate watching netflix original movies\"\n",
    "tokens=text_cleaner(test_ip)\n",
    "vect_neg=t2v(tokens,feat_selection)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svml=svm.LinearSVC()\n",
    "model = svml.fit(vector_feat,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype='<U1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict([vect_pos])\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0'], dtype='<U1')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict([vect_neg])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1-based feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1143, 9)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "lsvc = LinearSVC(C=0.01, penalty=\"l1\", dual=False).fit(vector, y)\n",
    "t_model = SelectFromModel(lsvc, prefit=True)\n",
    "X_new = t_model.transform(vector)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 1, 0, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_new[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "svml=svm.LinearSVC()\n",
    "model = svml.fit(X_new,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip=\"I love watching netflix original movies\"\n",
    "tokens=text_cleaner(test_ip)\n",
    "vect_pos=t2v(tokens,vocabulary)\n",
    "vect_pos = t_model.transform(np.asarray([vect_pos]))\n",
    "vect_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0'], dtype='<U1')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict(vect_pos)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip=\"I hate watching netflix original movies\"\n",
    "tokens=text_cleaner(test_ip)\n",
    "vect_neg=t2v(tokens,vocabulary)\n",
    "vect_neg = t_model.transform(np.asarray([vect_neg]))\n",
    "vect_neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0'], dtype='<U1')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict(vect_neg)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensional Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA based dimensional reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1143, 200)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=200)\n",
    "X_new=pca.fit_transform(vector)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svml=svm.LinearSVC()\n",
    "model = svml.fit(X_new,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip=\"I love watching netflix original movies\"\n",
    "tokens=text_cleaner(test_ip)\n",
    "vect_pos=t2v(tokens,vocabulary)\n",
    "vect_pos = pca.transform(np.asarray([vect_pos]))\n",
    "vect_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype='<U1')"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict(vect_pos)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip=\"I hate watching netflix original movies\"\n",
    "tokens=text_cleaner(test_ip)\n",
    "vect_neg=t2v(tokens,vocabulary)\n",
    "vect_neg = pca.transform(np.asarray([vect_neg]))\n",
    "vect_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['0'], dtype='<U1')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict(vect_neg)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD based dimensional reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1143, 200)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=200, n_iter=7, random_state=42)\n",
    "X_new=svd.fit_transform(vector)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gyanendro/anaconda3/lib/python3.7/site-packages/sklearn/svm/base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svml=svm.LinearSVC()\n",
    "model = svml.fit(X_new,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip=\"I love watching netflix original movies\"\n",
    "tokens=text_cleaner(test_ip)\n",
    "vect_pos=t2v(tokens,vocabulary)\n",
    "vect_pos = svd.transform(np.asarray([vect_pos]))\n",
    "vect_pos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ip=\"I hate watching netflix original movies\"\n",
    "tokens=text_cleaner(test_ip)\n",
    "vect_neg=t2v(tokens,vocabulary)\n",
    "vect_neg = svd.transform(np.asarray([vect_neg]))\n",
    "vect_neg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype='<U1')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict(vect_pos)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['1'], dtype='<U1')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = model.predict(vect_neg)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
